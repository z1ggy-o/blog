---
title: "TOCS'13 - Spanner: Googleâ€™s Globally Distributed Database"
categories: 
  - paper notes
tags: 
  - distributed-systems
toc: false
isCJKLanguage: false
date: 2022-07-31 05:40:38
draft: false
permalink: /pages/c58ed1/
author: 
  name: gyzhu
  link: https://github.com/z1ggy-o
---

## Motivation

- Some applications need relation data model and strong consistency which BigTable cannot gives.
    
- So, Google want to develop a system that focuses on managing cross-datacenter replicate data with database features.
    

## Contribution

- Provides a globally distributed database that shards data across many sets of Paxos state machine in datacenters.
    
- Provide SQL-like interface, strong consistency, and high read performance.
    

## Solution

- Replication
    
    - Use Paxos to replicate data to several nodes, which provides higher availability.
        
- Local Transactions (within a paxos group)
    
    - Each Paxos group leader has a lock table to implement concurrency control.
        
    - Spanner chooses to use 2PL as the protocol because it is designed for long-lived txns, which performs poor under optimistic protocols.
        
- Distributed Transactions (across paxos groups)
    
    - Each Paxos group leader has a transaction manager to support distributed txns.  
        The transaction manager is used as a participant leader.
        
    - Spanner uses 2PC to support distributed txns. Paxos group leaders become the coordinator and participants.
        
- Snapshot Isolation
    
    - For better read performance, Spanner uses MVCC to enable read without locking
        
- TrueTime
    
    - MVCC needs timestamp. Since Spanner is a globally distributed system, we cannot use logical timestamp (otherwise, the centric timestamp manager can be the bottleneck), we also cannot use physical time (not accurate).
        
    - To resolve this problem, Spanner introduces TrueTime, which is a time range based on physical time instead of a single time point.
        
    - This time range can cover the error of physical clocks from different datacenters. We use GPS and atomic clocks to sync timestamp on different datacenters. So we can ensure the txn order without a centric timestamp manager.
        

## Evaluation

- Micro benchmark that shows the performance of replication, transactions and availability on a setup that the network distance between each other is less than 1ms. (A common layout, most apps do not need to distribute all data worldwide)
    
- Analysis of a real world workload, F1.
    

## The main takeaways

- A real cool system that shows how to combine techniques from different fields together.
